1 - Basicamente é a implementação de uma meta heurística pra encontrar a solução
ótima (ou uma solução boa o suficiente) sobre um problema np completo.
Além disso, tem que modelar o problema matematicamente no GLPK.

4 partes:
- Formulação matemática
- Implementação da meta heurística
- Relatório (de cunho bem científico, I like it)
- Apresentação

2 - Formulação matemática
Separar dados (instância) da formulação.
Deve conter um arquivo .mod e arquivos de dados das instâncias no padrão GLPK

- Implementação
Qualquer linguagem open source sem bibliotecas proprietárias.
Cuidar com:
- Documentação, legibilidade.
- Parâmetros recebidos via linha de comando (principalmente a semente para o
  gerador de números aleatórios)
- A eficiência da implementação e a qualidade das soluções serão consideradas na
  avaliação.
- Não pode parar a execução por um critério de tempo. Usar: número de iterações,
  iterações sem encontrar solução melhor, proximidade de limitante. Evitar que
  demore mais que 5 minutos.

Entregar o código fonte.
Entregar README informando como compilar/executar e dependências.

4 - Relatório
No máximo 6 páginas (sem capa e referências)
- Intro: explicação sobre a meta heurística
- Problema: problema e sua formulação matemática explicada. Ressaltar utilidade
  de cada restrição e a função objetivo.
- Descrição detalhada do algoritmo:
  Representação do problema
  Estruturas de dados
  Geração da solução inicial
  Vizinhanças e estratégia de seleção de vizinhos
  Parâmetros do método
  Critérios de parada
- Tabela de resultados:
  Relaxação linear do GLPK com a formulação ?????????
  Valor da melhor sol inteira encontrada pelo GLPK com a nossa formulação
  Runtime
  Valor médio da solução inicial do algoritmo
  Valor médio da melhor solução encontrada
  Desvio padrão das melhores soluções
  Runtime médio
  Desvio percentual médio das soluções obtidas pelo nosso alg com relação à
melhor solução conhecida
- Análise dos resultados
- Conclusões
- Refs

Usar média de 10 execuções pra cada instância.
Execuções feitas com uma seed aleatória diferente (ver mais)

DICAS:
- Analisar bem a complexidade do algoritmo considerando as EDs
- Como perder pontos, do Ritt (VER)
- Cheat sheet do Ritt sobre o GLPK

Sobre Simulated Annealing:
É a mesma coisa que o hill climbing, mas com a questão de permitir escolhas
menos boas pra garantir que não se está preso em um ótimo local:
Eles chamam isso de slow cooling, que é a diminuição da probabilidade de aceitar
soluções piores conforme o espço de soluções é explorado. A dúvida ainda é como
é calculada essa probabilidade.

estado_atual = estado_random(); // sol inicial aleatória

para t : 1 ... condição de parada {
  temp--; // ou decresce de alguma outra forma, não necessariamente -1

  se temp == 0
    retorna estado_atual // pq ele é a última melhor escolha encontrada

  para i : 1 ... N { // TODO verificar o que pode ser esse N
    (1) estado_candidato = vizinho_random(estado_atual);

    diff_estado = valor(estado_candidato) - valor(estado_atual)
    // valor é dado pelo makespan que essa solução tem

    se diff_estado < 0 // significa que o vizinho encontrado tem menor makespan
      estado_atual = estado_candidato // novo melhor estado
    senão
      estado_atual = estado_candidato com prob E relacionada à temp ****
  }
}

**** Isso é justamente a probabilidade/chance que se dá para escolhas ruins. Não
foi possível encontrar uma solução melhor em (1), mas se a temperatura temp for
alta o suficiente, pode-se permitir uma escolha menos boa a partir da solução
atual ótima.

Então, trazendo pro escopo do trabalho:

FLOWSHOP: Permutational Flowshop Scheduling Problem

- n tarefas a serem processadas
a serem alocadas em
- m máquinas disponíveis
- tempos de processamento Tir > 0
      1 <= i <= n
      1 <= r <= m

Restrições:
- tarefas devem ser processadas em sequência (há ordem de execução)
- cada tarefa deve ser processada em todas as máquinas
- processamento sem preempção

Objetivo:
Minimizar o makespan, que é o tempo total de execução.
Solução:
Ordem de alocação das tarefas às máquinas

Ou seja, traduzindo o que diz na wikipedia:
There are m machines and n jobs. Each job contains exactly m operations. The i-th
operation of the job must be executed on the i-th machine. No machine can perform
more than one operation simultaneously. For each operation of each job, execution
time is specified.
Operations within one job must be performed in the specified order. The first
operation gets executed on the first machine, then (as the first operation is
finished) the second operation on the second machine, and so on until the m-th
operation. Jobs can be executed in any order, however. Problem definition implies
that this job order is exactly the same for each machine. The problem is to
determine the optimal such arrangement, i.e. the one with the shortest possible
total job execution makespan.

Cada job ta dividido em m pedaços pra que cada pedaço seja executado em uma das
m máquinas. Esses pedaços devem ser executados sequencialmente, respeitando a
ordem em que eles aparecem no job. O pedaço m deve ser executado na máquina m
antes de o pedaço m+1 ser executado na máquina m+1.
O objetivo é que o tempo total de execução do m-gésimo pedaço do n-gésimo job
seja executado no menor tempo possível na máquina m. Ou seja, reduzir o tempo de
execução de todos os pedaços de todas as aplicações em suas devidas máquinas.


Okay, useful links:
https://en.wikipedia.org/wiki/Flow_shop_scheduling
http://www.diegm.uniud.it/schaerf/SAS/Projects/FlowShop.pdf
TODO entender cada detalhe desse último pdf


03/06/19

Então, vendo os dados que tão no github, parece que não tem nada disso de
operações dentro das tasks, é só as tasks mesmo, nem tempo de chegada das tasks
não tem.

Então a gente tem que decidir qual vai ser a nossa métrica pra decidir qual task
escalonar primeiro. Por ex, a gente pode escolher a média do tempo de execução
da tarefa nas máquinas como métrica. Primeiro escolher a tarefa que tem a menor
média de execução entre as máquinas e escalonar ela, depois escolher a segunda
melhor, e assim vai. Daí praquele lance de escolher vizinhos piores numa dada
frequência x, a gente pode dizer que ao invés de escolher o próximo melhor tempo
médio entre as tarefas, podemos pegar a quinta tarefa com melhor tempo médio,
etc.

Also, foi debatido que sepa o melhor tempo médio não seja uma boa, então isso
pode ir pro relatório.

TODO ler sobre programação dinâmica.


09/06/19

Restrições:
Cri : tempo de completude da tarefa i na máquina r.

C1i >= T1i : na primeira máquina, o tempo de completude da tarefa i vai ser no
mínimo maior que o tempo que a tarefa i levaria pra executar.

Cri - Cr-1,i >= Tri o tempo de completude da tarefa i na máquina r menos o tempo
de completude da tarefa r na máquina anterior tem que ser no mínimo maior que o
tempo de executar i.

Cri - Crk + PDik >= Tri :
Dik: se a tarefa i é executada antes da k

15/06/19

Lendo sobre Simulated Annealing:
Slow cooling, que é a diminuição da probabilidade de aceitar
soluções piores conforme o espço de soluções é explorado. A dúvida ainda é como
é calculada essa probabilidade.
Tem essa probabilidade de Boltzman-Gibbs: after the generation of a random
initial solution, the move from the current solution S to a neighborhood
solution S* depends on the probability P of the Boltzman-Gibbs distribution of
the objective function difference. Besides, this transition is accepted if the
following condition is satisfied:

P = mim { 1, e^( (-deltaF) / T ) } >= omega

-deltaF = F(S) - F(S*) is the difference between the objective function of the
two states.
omega is a randomly generated number in the interval [0, 1]
T current control parameter in the process

okay but where do those values come from? How do we choose omega and how do we
set this temperature?

Apparently, the parameter T have to decrease logarithmically with the time.
The proposed algorithm:
By https://bit.ly/2Fcgdh0

Uma coisa que me chamou atenção no algoritmo proposto é que eles usam como
função F o makespan que cada solução tem. Por ex, criei a sol inicial S, que
consiste em uma ordem de execução das tarefas. O valor dessa solução é o cálculo
do makespan dada essa ordem de execução. Quando eu gerar uma sol vizinha, eu
avalio o makespan dela e vejo se é melhor.

Pra geração não aleatória de um vizinho:
Tenho as tarefas A, B, C, D e E. Na geração aleatória elas foram escalonadas de
modo: B, D, C, A, E. Uma forma de gerar uma solução vizinha a isso seria por ex
calcular a média de tempos de execução das tarefas.
Digamos que as médias sejam as seguintes:
A = 10
B = 12
C = 11
D = 19
E = 13
No caso acima, eu poderia colocar no lugar do B a tarefa que tivesse a menor
média de execução.

Dado isso, eu teria um algoritmo que gera uma sol inicial aleatória, os vizinhos
são gerados por meio dessa heurística de menor tempo médio de execução, e a
compração entre uma solução e outra é justamente o quão bom se tornou o
makespan delas.

No algoritmo deles, eles têm um número máximo de iterações como condição de
parada. O que é e como definir esse número máximo de iterações?

IDEIA:

initialize max_iter, omega, T
generate the initial population P(n) consisting of a random sequence of jobs
calculates the current population's makespan (that's the oldbest solution)
while the max_iter has not been attained
  generate new solution based on some heuristic
  if new_sol <= oldbest (makespan is smaller)
    oldbest = new_sol
  else
    calculate x = exp[ - (new_sol - oldbest) / T] // what's this?
    if x > omega:
      oldbest = new_sol
      reduce temperature by setting T = alpha*T // wtf is this alpha?


Apparently, alpha is the cooling rate parameter. So wtf is omega?
It looks like omega is just a randomly generated number between [0, 1] ???

In the paper, they present a methodology to define the best values for the
parameters. But it seems too difficult to me, so I'll just use the brute force
method kk
At least I can use their results:
max_iter = 600 (but i guess this one is only related to their approach using GA)
cool rate: 0.5
temperature: 700

No artigo https://www.academia.edu/25955854/Simulated_annealing_for_permutation_flow-shop_scheduling
ele propõe várias heurísticas. EScolher e citar elas.
Tem uma anotada no caderno.
Uma ideia seria: gera com base na média, mas lá pelas tantas só pegar e trocar
duas tarefas h e i de lugar, sendo que h < i. A tarefa h teve menor makespan que
a tarefa i, mas dá pra pegar e só trocar elas duas pra ver no que resulta.


17/06/19

De qualquer forma, o que eu tenho que fazer primeiro é parsear um daqueles
arquivos de entrada.
Depois disso posso implementar a geração aleatória de vizinhos, e depois disso a
geração com base no tempo médio de execução, a nossa função F inicial.

Qual a melhor ED em python pra fazer um store da entrada?
Pedro:
dicio do tipo:
{ "task1" : [7, 8, 34, 11, 20 ...]  }


Algoritmos aqui:
https://www.localsolver.com/docs/last/exampletour/flowshop.html
https://github.com/rtshadow/flowshop
https://github.com/suyunu/Flow-Shop-Scheduling
https://github.com/topics/scheduling-problem
https://github.com/topics/simulated-annealing?after=Y3Vyc29yOjMw&o=desc&s=stars&utf8=%E2%9C%93
